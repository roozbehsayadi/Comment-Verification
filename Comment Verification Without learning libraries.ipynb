{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes without library\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from hazm import *\n",
    "from cleantext import clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train_df['comment'] = train_df['comment'].fillna('XXXXXXXX')\n",
    "test_df['comment'] = test_df['comment'].fillna('XXXXXXXX')\n",
    "train_df['title'] = train_df['title'].fillna('XXXXXXXX')\n",
    "test_df['title'] = test_df['title'].fillna('XXXXXXXX')\n",
    "\n",
    "train_spam_count = train_df.query('verification_status==0')['verification_status'].count()\n",
    "train_ham_count = train_df.query('verification_status==1')['verification_status'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'stop_words') as f:\n",
    "    stop_words = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_extraction(sentence):\n",
    "    normalizer = Normalizer()\n",
    "    sentence = normalizer.normalize(sentence)\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = word_tokenize(sentence)\n",
    "    words = [w for w in words if len(w) > 1 ]\n",
    "    words = [w.lower() for w in words]\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # words = [re.split(r'([a-zA-Z]+)', w) for w in words]\n",
    "    cleaned_words = []\n",
    "    for word in words:\n",
    "        cleaned_words_temp = re.split(r'([a-zA-Z]+)', word)\n",
    "        cleaned_words.extend(word for word in cleaned_words_temp if len(word) >= 1)\n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        # print(sentence)\n",
    "        w = word_extraction(sentence)\n",
    "        words.extend(w)\n",
    "    words = sorted(list(set(words)))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comments_ham_comment = train_df.query('verification_status==1')['comment'].values.tolist()\n",
    "comments_ham_title = train_df.query('verification_status==1')['title'].values.tolist()\n",
    "comments_spam_comment = train_df.query('verification_status==0')['comment'].values.tolist()\n",
    "comments_spam_title = train_df.query('verification_status==0')['title'].values.tolist()\n",
    "\n",
    "comments_test_comment = test_df['comment'].values.tolist()\n",
    "comments_test_title = test_df['title'].values.tolist()\n",
    "\n",
    "all_comments = []\n",
    "all_comments.extend(comments_spam_comment)\n",
    "all_comments.extend(comments_spam_title)\n",
    "all_comments.extend(comments_ham_comment)\n",
    "all_comments.extend(comments_ham_title)\n",
    "all_comments.extend(comments_test_comment)\n",
    "all_comments.extend(comments_test_title)\n",
    "\n",
    "vocabulary = tokenize(all_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_frequencies = {}\n",
    "\n",
    "zero_temp = np.zeros((len(vocabulary), 1), dtype=np.int8).tolist()\n",
    "\n",
    "comments_ham_df = train_df.query('verification_status==1')[['comment', 'title']]\n",
    "\n",
    "# ham\n",
    "counter = 0\n",
    "for word in vocabulary:\n",
    "    comments_mask = comments_ham_df.comment.str.contains(word)\n",
    "    titles_mask = comments_ham_df.title.str.contains(word)\n",
    "    aggregate_mask = (comments_mask | titles_mask)\n",
    "    if (aggregate_mask.value_counts()[False] != comments_ham_df.shape[0]):\n",
    "        count = (comments_mask | titles_mask).value_counts()[True]\n",
    "    else:\n",
    "        count = 0\n",
    "    count /= train_ham_count\n",
    "    ham_frequencies[word] = count\n",
    "    counter += 1\n",
    "    if (counter % 100 == 0):\n",
    "        print(counter / len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame.from_dict(ham_frequencies, orient='index')).to_csv(r'ham_frequencies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_frequencies = {}\n",
    "\n",
    "zero_temp = np.zeros((len(vocabulary), 1), dtype=np.int8).tolist()\n",
    "\n",
    "comments_spam_df = train_df.query('verification_status==0')[['comment', 'title']]\n",
    "\n",
    "#spam\n",
    "counter = 0\n",
    "for word in vocabulary:\n",
    "    comments_mask = comments_spam_df.comment.str.contains(word)\n",
    "    titles_mask = comments_spam_df.title.str.contains(word)\n",
    "    aggregate_mask = (comments_mask | titles_mask)\n",
    "    if (aggregate_mask.value_counts()[False] != comments_spam_df.shape[0]):\n",
    "        count = (comments_mask | titles_mask).value_counts()[True]\n",
    "    else:\n",
    "        count = 0\n",
    "    count /= train_spam_count\n",
    "    spam_frequencies[word] = count\n",
    "    counter += 1\n",
    "    if (counter % 100 == 0):\n",
    "        print(counter / len(vocabulary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame.from_dict(spam_frequencies, orient='index')).to_csv(r'spam_frequencies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_frequencies = pd.read_csv(r'ham_frequencies.csv').T.to_dict('list')\n",
    "spam_frequencies = pd.read_csv(r'spam_frequencies.csv').T.to_dic('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating answers for test\n",
    "answers = {}\n",
    "for index, row in test_df.iterrows():\n",
    "    # print(row['title'], 'alwkdlawkjdlkawj', row['comment'])\n",
    "    current_id = row['id']\n",
    "    current_comment = row['comment']\n",
    "    current_title = row['title']\n",
    "    words = word_extraction(current_comment)\n",
    "    words.extend(word_extraction(current_title))\n",
    "    spam_prob = 1\n",
    "    ham_prob = 1\n",
    "    for word in words:\n",
    "        if (spam_frequencies[word] != 0):\n",
    "            spam_prob *= spam_frequencies[word]\n",
    "        if (ham_frequencies[word] != 0):\n",
    "            ham_prob *= ham_frequencies[word]\n",
    "    if (spam_prob > ham_prob):\n",
    "        answers[current_id] = 0\n",
    "    else:\n",
    "        answers[current_id] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_df = pd.DataFrame(list(answers.items()), columns=['id', 'verification_status'])\n",
    "answers_df = answers_df.set_index('id')\n",
    "answers_df.to_csv(r'ans.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
